---
title: 倫理與治理趨勢
description: 初探目前關於倫理與治理議題的挑戰與趨勢
---

> 審議業師 小工｜2025年編纂

## 先談談對齊

???+ question "想一想"

	- 誰有權決定什麼是好的「倫理」與「價值觀」？
	* 為了確保人工智慧的公平和負責任使用，你認為哪些倫理原則應該被優先考慮？
	* 在人工智慧治理面向上，公民社會可以發揮什麼作用？



---

??? info "從對齊開始聊聊"
	=== "對齊的概念與方式"

		在人工智慧領域，特別是針對大型語言模型 (LLMs)，**對齊 (Alignment)** 指的是使 AI 系統的行為、目標和價值觀與人類的意圖、倫理規範和期望相一致的過程。一個「對齊」良好的 AI 系統應該能夠產生有益的、真實的、無害的回應，並遵循人類的指令，同時避免產生有害、誤導性或不道德的內容。

	=== "重要性"

		隨著 LLMs 能力的不斷提升，它們在各個領域的應用也越來越廣泛。然而，這些模型在訓練過程中學習的是預測文本序列的下一個單詞，這並不一定保證它們的輸出總是符合人類的期望。未經良好對齊的模型可能產生：

		* **有害內容 (Harmful Content):** 例如仇恨言論、暴力煽動、歧視性語言等。
		* **不實資訊 (False Information):** 模型可能會生成看似合理但不真實的陳述（即「幻覺」）。
		* **不安全的行為 (Unsafe Behavior):** 在某些情境下，模型可能會產生鼓勵或促成危險行為的輸出。
		* **與人類意圖不符的回應 (Responses Misaligned with Human Intent):** 模型可能誤解指令、產生不相關或無用的回應。

		因此，「對齊」是確保 AI 技術能夠安全、可靠且有益地服務於人類的關鍵步驟。

	=== "怎麼對齊"

		=== "RLHF"
			- 人類回饋強化學習 (Reinforcement Learning from Human Feedback, RLHF)是目前最主流且有效的方法之一
			- 主要步驟：
	    		* **收集人類偏好數據 (Human Preference Data Collection):** 對於模型針對同一輸入產生的不同輸出，讓人類評估者進行比較和排序，指出哪個輸出更符合人類的期望（例如，更有幫助、更真實、更無害）。
	    		* **訓練獎勵模型 (Reward Model Training):** 使用收集到的人類偏好數據訓練一個獎勵模型。這個模型學習預測人類對於不同模型輸出的偏好分數。對於更符合人類偏好的輸出，獎勵模型會給予更高的分數。
	    		* **使用強化學習優化 LLM (LLM Optimization with Reinforcement Learning):** 使用獎勵模型作為獎勵信號，通過強化學習算法（例如 Proximal Policy Optimization, PPO）微調原始的 LLM。目標是讓 LLM 生成的輸出能夠獲得獎勵模型的更高評分，從而使其行為更符合人類的偏好。
		=== "DPO"
			- **直接偏好最佳化** (Direct Preference Optimization, DPO)是一種比 RLHF 更簡化的對齊方法。
			- DPO 試圖直接從人類偏好數據中學習一個策略（即微調後的 LLM），而無需顯式地訓練一個獎勵模型
			- 透過特殊的損失函數，直接最佳化模型以生成人類更偏好的輸出。
			- 相較於 RLHF 更具訓練穩定性和計算成本的優勢。

		=== "提示工程和指令微調"
			=== "提示工程"
	    		- 透過精心設計的提示，引導模型產生更符合期望的輸出
	    		- 例如：使用明確的指令、提供上下文、指定輸出格式、要求模型解釋其推理過程等。
			=== "指令微調"
				- 使用包含指令和對應高品質輸出的資料集對模型進行微調
				- 使模型更能理解和遵照人類的指令，並產生更符合指令意圖的回應
				- 可以與 RLHF 或 DPO 結合使用，進一步提升對齊效果。
		=== "基於規則和約束"
	    	* 在模型的輸出生成過程中或生成後，應用一系列規則或約束來過濾或修改不符合要求的內容
	    	- 例如：使用關鍵詞過濾器、安全列表、或強制模型遵循特定的格式或風格。
	=== "挑戰"
		- **人類偏好的主觀性和多樣性:** 不同人對於什麼是「有益」、「真實」或「無害」可能有不同的看法。
		- **定義和衡量「對齊」的困難:** 如何客觀地評估一個 AI 系統的對齊程度是一個複雜的問題。
		* **對齊過程中的潛在偏見引入:** 人類評估者自身的偏見可能會影響獎勵模型的訓練和最終模型的行為。
		* **對齊的權衡:** 過於嚴格的對齊可能會限制模型的創造性和表達能力。
		* **長期對齊和價值觀演變:** 如何確保 AI 系統的價值觀能夠隨著時間和社會的發展而演變是一個長期的挑戰。

還有一些其他的比如「超對齊」等，在此先不贅述。

---

## 常見倫理挑戰

<div class="grid cards" markdown>

-    __偏見與公平性__

    ---

    人工智慧模型可能會延續甚至放大訓練數據中存在的現有社會偏見，導致在應用領域出現不公平或歧視的結果。


-    __透明度與可解釋性__

    ---

    一些複雜的人工智慧模型（尤其是深度學習）的「黑箱」性質使得難以理解它們的決策過程，這對信任和當責構成挑戰。

-   __責任歸屬__

    ---

    當人工智慧系統做出錯誤或造成損害時，確定責任歸屬（開發者、部署者、使用者或系統本身）是一個複雜的問題


-    __隱私與監控__

    ---

    人工智慧驅動的監控技術引發了關於隱私侵犯和公民自由的擔憂。

-    __就業與經濟影響__

    ---

    人工智慧的廣泛採用可能會導致某些工作崗位的流失，並需要重新思考教育和勞動力發展策略。

-    __著作權利__

    ---

    隨著人工智慧具備產製內容的能力，其著作權歸屬有待釐清；另一方面訓練模型所需的資料並非公開，是否涉及版權資料的運用以及其是否可視為合理使用，尚待確認。



</div>

## 回到治理

!!! note "關於AI治理"
	=== "什麼是AI治理"
		AI治理是指為了確保人工智慧的開發、部署和使用符合倫理、法律、社會價值觀和公共利益，而建立的框架、原則、政策、法律、標準和實踐。其目標是最大化 AI 的益處，同時最小化其潛在風險和負面影響。

		有效的 AI 治理有助於建立公眾信任、促進創新、確保公平競爭、保護基本權利和促進社會福祉。

	=== "重要性"
		隨著 AI 技術的快速發展和在各個領域的廣泛應用，有效的治理變得重要，以應對以下挑戰
		=== "倫理風險"
			例如：偏見、歧視、缺乏透明度、責任歸屬不明等
		=== "法律風險"
			例如：數據隱私侵犯、智慧財產權爭議、產品責任等
		=== "社會影響"
			例如：就業結構改變、社會不平等加劇、資訊操縱等
		=== "安全風險"
			例如：惡意使用 AI、對抗性攻擊、模型污染等
		=== "經濟影響"
			例如：市場壟斷、公平競爭問題等
	=== "主要面向"
		=== "倫理原則與指導方針"
			制定 AI 開發和使用的倫理原則，例如公平、透明、可解釋、負責任、尊重人類自主性等。許多組織和政府都發布了相關的倫理指導方針，但將其轉化為具體實踐仍然是一個挑戰。
		=== "法律與法規框架"
			制定或調整現有法律法規，以應對 AI 帶來的法律挑戰。這可能包括資料保護、消費者權益、反歧視、智慧財產權等方面的法律。目前，許多國家和地區仍在探索和制定相關的法律框架。
		=== "政策與戰略"
			政府制定國家級 AI 發展戰略和政策，以促進 AI 創新，同時管理其風險。這些政策可能涉及研發投資、人才培養、基礎設施建設、標準制定等方面。
		=== "標準與技術規範"
			開發 AI 系統的技術標準和規範，以確保 AI 產品和服務的品質和安全性。如：資料品質、模型可解釋性、安全性和可靠性的標準
		=== "監管與執法機制"
			建立監管機構和機制，監督 AI 的開發和使用，並對違規行為執法。這需要考慮如何有效監管快速發展的 AI 技術。
		=== "國際合作"
			AI 的發展和應用具全球性，需加強國際合作，共同應對 AI 帶來的挑戰，促進全球範圍的協調治理
		=== "公眾參與及教育"
			提高公眾對 AI 的認識和理解，促進公眾參與 AI 治理的討論和決策過程
	=== "主要挑戰"
		=== "技術快速發展"
			AI 技術的發展速度遠超治理框架的制定速度，導致監管滯後的問題。
		=== "定義衡量困難"
			難以清晰地定義和衡量許多與 AI 治理相關的概念，比如：「公平」、「透明」和「責任」。
		=== "跨領域和部門"
			AI 的應用涉及眾多領域和部門，需要跨領域和跨部門的協調與合作，往往面臨挑戰。
		=== "全球協調困難"
			不同國家和地區價值觀、法律體系和發展階段存在差異，使得全球範圍的 AI 治理協調非常困難。
		=== "權衡創新與監管"
			如何在促進 AI 創新和有效監管其風險之間取得平衡是一個核心挑戰。過度監管可能會扼殺創新，而監管不足則可能導致風險失控。
		=== "執法挑戰"
			監管 AI 系統的複雜性和全球性，以及缺乏專業知識和資源
		=== "公眾理解不足"
			公眾對 AI 技術的理解程度有限，提升有效參與治理討論和決策的難度
		=== "資料依賴性"
			AI 系統的性能和公平性高度依賴於訓練資料的品質和偏誤；此外資料的獲取、使用和管理也涉及複雜的倫理和法律問題。
		=== "超級人工智慧的潛在風險"
			雖然尚屬未來，但對超級人工智慧的潛在風險進行治理和預防，是一個更為複雜和長期的挑戰

## 一些可能的方式

> 現正進行中的方式

* **開發可解釋人工智慧 (Explainable AI, XAI) 技術:** 研究使人工智慧決策過程更透明的方法。
* **實施嚴格的偏見檢測和緩解策略 (Implementing Robust Bias Detection and Mitigation Strategies):** 開發工具和方法分辨並減少人工智慧模型中的偏見。
* **建立明確的責任框架 (Establishing Clear Accountability Frameworks):** 制定法律和監管框架，釐清人工智慧系統造成損害時的責任歸屬。
* **推動倫理設計原則 (Promoting Ethical Design Principles):** 將倫理考量納入人工智慧系統的設計和開發過程。
* **加強公眾意識和教育 (Enhancing Public Awareness and Education):** 提高公眾對人工智慧的潛在益處和風險的認識。
* **多方利益相關者合作 (Multi-stakeholder Collaboration):** 促進政府、產業、學術界和公民社會之間的合作，共同應對人工智慧的倫理和治理挑戰。


